---
title: "Tidymodels"
output: html_notebook
---

```{r}
library(tidymodels)
library(tidyverse)

# colnames(total_19)
gravity <- total_19%>% 
  filter(!is.na(grav)) %>% 
  filter(grepl("^35", dep)) %>% 
  mutate(grav_or_not = 
        case_when(
        grav == "2" | grav == "3" ~ "1",
        TRUE ~ "0"
        )
  ) %>% 
  mutate(age = 2019-an_nais) %>% 
  mutate(grav_or_not = as.factor(grav_or_not),
         sexe = as.factor(sexe),
         trajet = as.factor(trajet),
         secu_or_not = as.factor(secu_or_not),
         secu_level = as.factor(secu_level)
         ) %>%
  select(grav_or_not, sexe, age, secu_or_not,trajet, secu_level, lat, long)

gravity %>% group_by(grav_or_not) %>% 
  summarise(n=n())
  
glimpse(gravity)
colnames(total_19)



```



# il faudrait faire quelques ggplot
```{r}

```

# Définition de trainning set et test set
```{r}

set.seed(1234)


ames_split <- initial_split(gravity, prob = 0.80)
ames_train <- training(ames_split)
ames_test <- testing(ames_split)
# ames_test_small <- ames_test %>% head(5)

dim(ames_train)
dim(ames_test)
colnames(ames_train)

```

# Définition de recette basique et visualisation de design matrice
```{r}
ames_rec <-
  recipe(grav_or_not ~ ., data = ames_train) %>%
#   step_other(UniqueCarrier, threshold = 0.01) %>%
#   # step_mutate(DepDelay = log(1-min(DepDelay)+DepDelay))%>%
#   step_mutate(DepDelay=log(1-min(DepDelay)+DepDelay))%>%
  step_dummy(all_nominal(), -grav_or_not)
  # %>%
#   step_interact( ~ DepDelay:starts_with("UniqueCarrier_") ) %>%
#   step_ns(DepDelay, deg_free = 20)

# ces lignes de code dessous pour visualiser la design matrice
ames_rec_prepped <- prep(ames_rec) # si je met pas de parametre data, il va prendre le meme que la recette

# hf_train_prepped <-  bake(ames_rec_prepped, new_data = NULL) # cest notre matrice_design
# hf_test_prepped <- bake(ames_rec_prepped, new_data = ames_test)

matrice_design <- bake(ames_rec_prepped, new_data = NULL)
matrice_design

```

# Fabriquer matrice_design_test pour neural_network
```{r}
ames_rec_test <-
  recipe(grav_or_not ~ ., data = ames_test) %>%
#   step_other(UniqueCarrier, threshold = 0.01) %>%
#   # step_mutate(DepDelay = log(1-min(DepDelay)+DepDelay))%>%
#   step_mutate(DepDelay=log(1-min(DepDelay)+DepDelay))%>%
  step_dummy(all_nominal(), -grav_or_not)
  # %>%
#   step_interact( ~ DepDelay:starts_with("UniqueCarrier_") ) %>%
#   step_ns(DepDelay, deg_free = 20)

# ces lignes de code dessous pour visualiser la design matrice
ames_rec_prepped_test <- prep(ames_rec_test) # si je met pas de parametre data, il va prendre le meme que la recette

# hf_train_prepped <-  bake(ames_rec_prepped, new_data = NULL) # cest notre matrice_design
# hf_test_prepped <- bake(ames_rec_prepped, new_data = ames_test)

matrice_design_test <- bake(ames_rec_prepped_test, new_data = NULL)
matrice_design_test
```


## Definition des modèles
# https://parsnip.tidymodels.org/reference/
# https://www.tmwr.org/workflow-sets.html

# Log
```{r}


log_model <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")


log_wflow <-
  workflow() %>%
  add_model(log_model) %>%
  add_recipe(ames_rec)
  
log_fit <- fit(log_wflow, ames_train)

log_pred <- ames_test %>%
  select(grav_or_not) %>%
  bind_cols(predict(log_fit, ames_test, type='prob')) 
# %>%
#   bind_cols(predict(lm_fit, ames_test, type = "pred_int")) # pour RandomForest il ny a pas de pred_int

log_pred

```

# LASSO/Ridge regression
# https://juliasilge.com/blog/lasso-the-office/

```{r}

```


# Ramdon forest
```{r}
  

library(ranger)
rf_model <-
  rand_forest(trees = 10) %>%
  set_engine("ranger") %>%
  set_mode("classification")

rf_wflow <-
  workflow() %>%
  add_recipe(ames_rec) %>% 
  add_model(rf_model)

# rf_fit et rf_pred n'est pas forcement necessaire à cette étape. on peut directement passer a la validation
rf_fit <- 
  fit(rf_wflow, ames_train)

rf_pred <- ames_test %>%
  select(grav_or_not) %>%
  bind_cols(predict(rf_fit, ames_test, type='prob')) 
   
rf_pred

```

# General Interface for Boosted Trees
```{r}
library(xgboost)

xgboost_model <- 
  parsnip::boost_tree(
    mode = "classification",
    trees = 10,
    sample_size =0.1
    # min_n = tune(),
    # tree_depth = tune(),
    # learn_rate = tune(),
    # loss_reduction = tune()
  ) %>%
    set_engine("xgboost", objective = "reg:squarederror")

xgboost_wf <- 
  workflows::workflow() %>%
  add_model(xgboost_model) %>%
  add_recipe(ames_rec)

bt_fit <-
  fit(xgboost_wf, ames_train)

bt_pred <- ames_test %>%
  select(grav_or_not) %>%
  bind_cols(predict(bt_fit, ames_test, type='prob'))

bt_pred

```



# linear support vector machines

```{r}
svm_linear() %>% 
  set_engine("LiblineaR") %>% 
  set_mode("classification") %>% 
  translate()

svm_res <- tune_grid(svm_mod, car_rec, resamples = folds, grid = 7)
svm_res


```

# Neural Network
```{r}

```


## Evaluation d'un modèle. En réalité on va utiliser tidymodels Workflows collection (plus tard)
# Validation croisée
# Combine metric functions :  https://yardstick.tidymodels.org/

```{r}

# ames_resampling <- vfold_cv(ames_train, v = 10,)
# # ici on peut aussi faire un bootstrapping
# 
# keep_pred <- control_resamples(save_pred = TRUE, save_workflow = TRUE)
# 
# rf_results <- rf_wflow %>% 
#   fit_resamples(resamples = ames_resampling,
#                 metrics = metric_set(accuracy, roc_auc, pr_auc),
#                 control = keep_pred)
# 
# 
# collect_metrics(rf_results)
# 
# assess_res <- collect_predictions(rf_results)
# 
# 
# assess_res %>%
#   select(.pred_class,grav_or_not) %>% 
#   group_by(.pred_class,grav_or_not) %>% 
#   summarise(n=n())
# 
# 
# 
# ### ici il faut creer une matrice
# 
# ggplot(data = assess_res, aes(x = grav_or_not, y = .pred_class)) +
#   geom_col(aes(fill = grav_or_not), width = 0.7)
# 

```

# Comparing models with resampling
# https://www.tmwr.org/compare.html

# Create a Collection of recipes



```{r}
ames_resampling <- vfold_cv(ames_train, v = 3,)
keep_pred <- control_resamples(save_pred = TRUE, save_workflow = TRUE)

interaction_rec <- 
  ames_rec %>% 
  step_interact( ~ starts_with("secu_or_not_"):starts_with("trajet_")) 

spline_rec <- 
  interaction_rec %>% 
  step_ns(lat, long, deg_free = 20)

preproc <- 
  list(basic = ames_rec, 
       interact = interaction_rec, 
       splines = spline_rec
  )




```

## Create a Collection of tidymodels Workflows
# https://workflowsets.tidymodels.org/

```{r}
chi_models <- 
   workflow_set(
      preproc = list( basic = ames_rec, 
                      interact = interaction_rec, 
                      splines = spline_rec),
      models = list(log = log_model,
                    rf=rf_model,
                    bt=xgboost_model),
                    # glmnet = regularized_spec, 
                    # cart = cart_spec, 
                    # knn = knn_spec),
      cross = TRUE
   )
chi_models



# regarder a quoi cela sert
# chi_models <- 
#    chi_models %>% 
#    anti_join(tibble(wflow_id = c("pca_glmnet", "filter_glmnet")), 
#              by = "wflow_id")



```

# Fit a collection of tidymodels Workflows

```{r}
chi_models <- 
  chi_models %>% 
  workflow_map(#"fit_resamples", 
               # Options to `workflow_map()`: 
               seed = 1101, verbose = TRUE,
               # Options to `fit_resamples()`: 
               resamples = ames_resampling, 
               # fn = "tune_grid",
               metrics = metric_set(accuracy,roc_auc, pr_auc),  
               control = keep_pred)



```

# recuperer les metrics

```{r}
collect_metrics(chi_models)

```

# recuperer les resultats de prediction (pour faire des visualisations ou Interpretable Machine Learning)

```{r}
collect_predictions(chi_models)
assess_res <- collect_predictions(chi_models)
assess_res
dim(assess_res)
```


```{r}
autoplot(chi_models, metric = "accuracy")
autoplot(chi_models, metric = "roc_auc")
autoplot(chi_models, metric = "pr_auc")

```


## Tunning
# Trois cas pour faire la tuning, la difficulté est de recupérer les paramètres tuning depuis un workflow_map (car show_best ne fonctionne qu'avec un tuning_grid (ie un model et un recette))

# Tuning le model
```{r}

xgboost_model_tune <- 
  parsnip::boost_tree(
    mode = "classification",
    trees = 10,
    min_n = tune(),
    tree_depth = tune(),
    learn_rate = tune(),
    loss_reduction = tune()
  ) %>%
  set_engine("xgboost", objective = "reg:squarederror")

mars_spec_tune <- 
   mars(prod_degree = tune()) %>%  #<- use GCV to choose terms
   set_engine("earth") %>% 
   set_mode("classification")

svm_r_spec_tune <- 
   svm_rbf(cost = tune(), rbf_sigma = tune()) %>% 
   set_engine("kernlab") %>% 
   set_mode("classification")

svm_p_spec_tune <- 
   svm_poly(cost = tune(), degree = tune()) %>% 
   set_engine("kernlab") %>% 
   set_mode("classification")

library(rules)
cubist_spec_tune <- 
   cubist_rules(committees = tune(), neighbors = tune()) %>% 
   set_engine("Cubist")

knn_spec_tune <- 
   nearest_neighbor(neighbors = tune(), dist_power = tune(), weight_func = tune()) %>% 
   set_engine("kknn") %>% 
   set_mode("classification")

cart_spec_tune <- 
   decision_tree(cost_complexity = tune(), min_n = tune()) %>% 
   set_engine("rpart") %>% 
   set_mode("classification")


library(baguette)
tidymodels_prefer()
bag_cart_spec <- 
   bag_tree() %>% 
   set_engine("rpart", times = 50L) %>% 
   set_mode("classification")


nnet_spec_tune <- 
   mlp(hidden_units = tune(), penalty = tune(), epochs = tune()) %>% 
   set_engine("nnet", MaxNWts = 2600) %>% 
   set_mode("classification")

nnet_param <- 
   nnet_spec_tune %>% 
   parameters() %>% 
   update(hidden_units = hidden_units(c(1, 27)))




```

```{r}
# normalized <- 
#    workflow_set(
#       preproc = list(basic = ames_rec), 
#       models = list(#SVM_radial = svm_r_spec_tune, 
#                     #SVM_poly = svm_p_spec_tune, 
#                     KNN = knn_spec_tune, 
#                     neural_network = nnet_spec_tune)
#    )
# 
# 
# 
# normalized %>% pull_workflow(id = "basic_neural_network")
# 
# normalized <- 
#    normalized %>% 
#    option_add(param = nnet_param, id = "basic_neural_network")
# normalized
# 
# all_workflows <- 
#    bind_rows(normalized) %>% 
#    # Make the workflow ID's a little more simple: 
#    mutate(wflow_id = gsub("(simple_)|(normalized_)", "", wflow_id))
# all_workflows
# 
# grid_ctrl <-
#    control_grid(
#       save_pred = TRUE,
#       parallel_over = "everything",
#       save_workflow = TRUE
#    )
# 
# grid_results <-
#    all_workflows %>%
#    workflow_map(
#       seed = 1503,
#       resamples = ames_resampling,
#       metrics = metric_set(accuracy, roc_auc, pr_auc), 
#       grid = 25,# grid search is applied to each workflow using up to 25 different parameter candidates donc 25 models 
#       control = grid_ctrl
#    )

# grid_ctrl <-
#    control_grid(
#       save_pred = TRUE,
#       parallel_over = "everything",
#       save_workflow = TRUE
#    )
# 
# full_results_time <- 
#    system.time(
#       grid_results <- 
#          all_workflows %>% 
#          workflow_map(seed = 1503, resamples = ames_resampling, grid = 25, 
#                       control = grid_ctrl, verbose = TRUE) 
#    )

# num_grid_models <- nrow(collect_metrics(grid_results, summarize = FALSE))
# num_grid_models
# grid_results
# 
# grid_results %>% 
#    rank_results() %>% 
#    filter(.metric == "pr_auc") %>% 
#    select(model, .config, roc_auc = mean, rank)
# 
# 
# 
# autoplot(
#    grid_results,
#    rank_metric = "pr_auc",  # <- how to order models
#    metric = "pr_auc",       # <- which metric to visualize
#    select_best = TRUE     # <- one point per workflow
# )
# 
# collect_metrics(grid_results)

```


# Tuning le recette

```{r}

spline_rec_tune <- 
  interaction_rec %>% 
  step_ns(lat, long, deg_free = tune())
parameters(spline_rec_tune)


```


# Cas 1 : Tuning modele et tuning recette, mais seulement 1 à 1 (pas de workflow_map)

```{r}

parameters(xgboost_model_tune)

xgboost_params

xgboost_grid <- 
  dials::grid_max_entropy(
    xgboost_params, 
    size = 60
  )

knitr::kable(head(xgboost_grid))

xgboost_wf <- 
  workflows::workflow() %>%
  add_model(xgboost_model_tune) %>%
  add_recipe(spline_rec_tune)

flow_param <- parameters(xgboost_wf)


xgboost_tuned <- tune::tune_grid(
  object = xgboost_wf,
  resamples = ames_resampling,
  grid = flow_param %>% grid_regular(levels = 2),
  metrics = yardstick::metric_set(accuracy,roc_auc, pr_auc),
  control = tune::control_grid(verbose = TRUE)
  )

flow_param %>% grid_regular(levels = 2)


xgboost_tuned %>%
  tune::show_best(metric = "roc_auc") %>%
  knitr::kable()
```

# Cas 2 : Tuning recette, ne pas tuning les modèles, travailler avec workflow
# https://www.tidyverse.org/blog/2021/03/workflowsets-0-0-1/

```{r}
preproc <- 
  list(basic = ames_rec, 
       interact = interaction_rec, 
       splines = spline_rec
  )


chi_models <- 
   workflow_set(
      preproc = list( #basic = ames_rec, 
                      #interact = interaction_rec, 
                      splines_tune = spline_rec_tune),
      models = list(log = log_model,
                    #rf=rf_model,
                    bt =xgboost_model),
      cross = TRUE
   )

chi_models
```


# A voir comment collecter les parameters tuning recette quand c'est passé par workflow_map

https://parsnip.tidymodels.org/articles/articles/Submodels.html


```{r}


chi_models <-
  chi_models %>%
  workflow_map(
    # Options to `tune_grid()`
    resamples = ames_resampling,
    grid = 20,
    metrics = metric_set(accuracy, roc_auc, pr_auc),
    # Options to `workflow_map()`
    seed = 3,
    verbose = TRUE
  )
chi_models


chi_models$result

rank_results(chi_models)
autoplot(chi_models)
autoplot(chi_models, select_best = TRUE)

rank_results(chi_models, rank_metric = "roc_auc", select_best = TRUE)

# splines_tune_log est classe en 1er

```

# passer à cas 1 pour recuperer le parameter degree
```{r}

wflow_param <-
  workflow() %>%
  add_recipe(spline_rec_tune) %>%
  add_model(log_model) %>%
  parameters()


log_Spline_tune <- 
  workflow() %>%
  add_recipe(spline_rec_tune) %>%
  add_model(log_model) %>%
  tune_grid(
    resamples = ames_resampling,
    grid = wflow_param %>% grid_regular(levels = 8),
    metrics = metric_set(accuracy,roc_auc, pr_auc)
  )

log_Spline_tune %>%
  tune::show_best(metric = "accuracy") %>%
  knitr::kable()

# selon la dernier commande degree 3 est legerement mieux
```

# Cas 3 : Tuning les modèles mais ne pas tuning la recette. Puis pousser directement la meilleure modèle dans workflow pour prédire. https://github.com/tidymodels/workflowsets/blob/main/vignettes/articles/tuning-and-comparing-models.Rmd



```{r}


chi_models <- 
   workflow_set(
      preproc = list( basic = ames_rec, 
                      #interact = interaction_rec,
                      spline_tune = spline_rec_tune
                      #splines = spline_rec
                      ),
      models =list(log = log_model,
                   # rf=rf_model,
                   # bt =xgboost_model_tune,
                   # SVM_radial = svm_r_spec_tune, 
                   # SVM_poly = svm_p_spec_tune, 
                   # KNN = knn_spec_tune, 
                   neural_network = nnet_spec_tune
                  ),
      cross = TRUE
   )
chi_models



chi_models <- 
  chi_models %>% 
  workflow_map(#"fit_resamples", 
               # Options to `workflow_map()`: 
               seed = 1101, verbose = TRUE,
               # Options to `fit_resamples()`: 
               resamples = ames_resampling, 
               grid = 20,
               metrics = metric_set(accuracy, roc_auc, pr_auc), 
               control = keep_pred)


```

# Ranking, trouver le meilleur modèle selon metric
```{r}
rank_results(chi_models, rank_metric = "roc_auc")  # splines_neural_network est classe en 1er, donc renseigner dans loption de best_results



# autoplot(chi_models, metric = "roc_auc")
autoplot(
   chi_models,
   rank_metric = "roc_auc",  # <- how to order models
   metric = "roc_auc",       # <- which metric to visualize
   select_best = TRUE     # <- one point per workflow
)

rank_results(chi_models, rank_metric = "roc_auc") %>% group_by(model,wflow_id) %>% summarise(n=n())
```

# Pousser le meilleur modèle dans le workflow puis faire la prédiction avec

# Pousser le modèle NN puis faire la prédiction
```{r}
################### Pousser neural_network ###########################
best_results <- 
   chi_models %>% 
   pull_workflow_set_result("splines_neural_network") %>% 
   select_best(metric = "roc_auc")

best_results


neural_network_test_results <- 
   chi_models %>% 
   pull_workflow("splines_neural_network") %>% 
   finalize_workflow(best_results) %>% 
   last_fit(split = ames_split)

neural_network_test_results %>% 
   collect_predictions()

################### Pousser KNN ###########################
best_results <- 
   chi_models %>% 
   pull_workflow_set_result("splines_KNN") %>% 
   select_best(metric = "roc_auc")
best_results


KNN_test_results <- 
   chi_models %>% 
   pull_workflow("splines_KNN") %>% 
   finalize_workflow(best_results) %>% 
   last_fit(split = ames_split)

KNN_test_results %>% 
   collect_predictions()

################### Pousser splines_SVM_poly ###########################
best_results <- 
   chi_models %>% 
   pull_workflow_set_result("splines_SVM_poly") %>% 
   select_best(metric = "roc_auc")
best_results


Boosting_test_results <- 
   chi_models %>% 
   pull_workflow("splines_SVM_poly") %>% 
   finalize_workflow(best_results) %>% 
   last_fit(split = ames_split)

Boosting_test_results %>% 
   collect_predictions()

################### Pousser splines_SVM_radial ###########################
best_results <- 
   chi_models %>% 
   pull_workflow_set_result("splines_SVM_radial") %>% 
   select_best(metric = "roc_auc")
best_results


Boosting_test_results <- 
   chi_models %>% 
   pull_workflow("splines_SVM_radial") %>% 
   finalize_workflow(best_results) %>% 
   last_fit(split = ames_split)

Boosting_test_results %>% 
   collect_predictions()



################### Pousser boosting ###########################
best_results <- 
   chi_models %>% 
   pull_workflow_set_result("splines_bt") %>% 
   select_best(metric = "roc_auc")
best_results


Boosting_test_results <- 
   chi_models %>% 
   pull_workflow("splines_bt") %>% 
   finalize_workflow(best_results) %>% 
   last_fit(split = ames_split)

Boosting_test_results %>% 
   collect_predictions()


################### Pousser RF ###########################
best_results <- 
   chi_models %>% 
   pull_workflow_set_result("splines_rf") %>% 
   select_best(metric = "roc_auc")
best_results


Boosting_test_results <- 
   chi_models %>% 
   pull_workflow("splines_rf") %>% 
   finalize_workflow(best_results) %>% 
   last_fit(split = ames_split)

Boosting_test_results %>% 
   collect_predictions()


################### Pousser log ###########################

splines_log_results <- 
  chi_models %>% 
  pull_workflow_set_result("splines_log")
splines_log_results

splines_log_workflow <- 
  chi_models %>% 
  pull_workflow("splines_log")
splines_log_workflow


splines_log_fit <- 
  splines_log_workflow %>% 
  finalize_workflow(tibble(prod_degree = 1)) %>% 
  fit(data = ames_train)
splines_log_fit

ames_test %>% 
  bind_cols(predict(splines_log_fit, ames_test, type='prob'))
```

## TO DO LIST
More and more models
# https://www.tmwr.org/workflow-sets.html
lasso
# https://juliasilge.com/blog/lasso-the-office/ 

## essaie d'inclure tune_grid dans workflow_map (seulement tuning le modèle, sans tuning la recette)
# https://www.tidyverse.org/blog/2021/03/workflowsets-0-0-1/

# (https://workflowsets.tidymodels.org/reference/workflow_map.html)

# (https://tune.tidymodels.org/reference/tune_grid.html)
